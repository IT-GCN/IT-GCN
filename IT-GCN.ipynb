{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN_new_input_HOSVD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0l1gisdX029",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/IT-GCN/')\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMH00Qn_3D0e",
        "colab_type": "text"
      },
      "source": [
        "### Re-implement graph.py to construct ajacency matrix A and normalized laplacian matrix L_hat (Graph.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIzG1hZs2u5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### re-implement graph.py to construct ajacency matrix A and normalized laplacian matrix L_hat\n",
        "\n",
        "import sklearn.metrics\n",
        "import sklearn.neighbors\n",
        "import scipy\n",
        "import numpy as np\n",
        "from scipy.sparse import rand\n",
        "\n",
        "\n",
        "### construct ajacency matrix\n",
        "def compute_dist(graph,k,metric):\n",
        "  dist = scipy.spatial.distance.pdist(graph,metric)\n",
        "  dist_square = scipy.spatial.distance.squareform(dist)\n",
        "  id_dist = np.argsort(dist_square)[:,:k+1]\n",
        "  dist_square.sort()\n",
        "  dist_square = dist_square[:,:k+1]\n",
        "  return dist_square,id_dist\n",
        "\n",
        "\n",
        "def build_ajacency(dist,idx):\n",
        "  M,k = dist.shape\n",
        "  sigma = np.mean(dist[:,-1])**2\n",
        "  dist = np.exp(-dist/sigma)\n",
        "\n",
        "  # construct sparse matrix\n",
        "  I = np.arange(0,M).repeat(k)\n",
        "  J = idx.reshape(M*k)\n",
        "  V = dist.reshape(M*k)\n",
        "  W = scipy.sparse.coo_matrix((V,(I,J)), shape = (M,M))\n",
        "  W.setdiag(0)\n",
        "  # construct symmetric matrix\n",
        "  bigger_index = W.T>W\n",
        "  # W = W - W.multiply(bigger_index) + W.T.multiply(bigger_index)\n",
        "  W = W + W.T.multiply(bigger_index)\n",
        "  assert(type(W)) is scipy.sparse.csr.csr_matrix\n",
        "  \n",
        "  return W\n",
        "\n",
        "def build_laplacian(W, normalized = True):\n",
        "  d = W.sum(axis = 0)\n",
        "  if not normalized:\n",
        "    D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
        "    L = D - W\n",
        "  else:   \n",
        "    d += np.spacing(np.array(0,W.dtype))\n",
        "    d = 1/np.sqrt(d)\n",
        "    D = scipy.sparse.diags(d.A.squeeze(),0)\n",
        "    I = scipy.sparse.identity(d.size,dtype= W.dtype)\n",
        "    L = I - D*W*D\n",
        "    \n",
        "  return L\n",
        "\n",
        "def rescale_L(L, l_max):\n",
        "  L = 2/l_max * L\n",
        "  I = scipy.sparse.identity(L.shape[0], format='csr', dtype=L.dtype)\n",
        "  L_hat = L - I\n",
        "  return L_hat\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDQPhJRR4eit",
        "colab_type": "text"
      },
      "source": [
        "### Re-implement model.py to Build GCN models (Model.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EluT-bL45zE-",
        "colab_type": "code",
        "outputId": "178c9ebe-0efe-4fb6-a3aa-b7aed8c8defb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import scipy.sparse\n",
        "import numpy as np\n",
        "import os, time, collections, shutil\n",
        "# import graph\n",
        "\n",
        "#NFEATURES = 28**2\n",
        "#NCLASSES = 10\n",
        "\n",
        "# Common methods for all models\n",
        "class base_model(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.regularizers = []\n",
        "\n",
        "    # High-level interface which runs the constructed computational graph.\n",
        "\n",
        "    # def output_feature(self, data, labels = None, sess = None):\n",
        "    #     sess = self._get_session(sess)\n",
        "    #     size = data.shape[0]\n",
        "    #     begin = 0\n",
        "    #     end = size\n",
        "    #     batch_data = np.zeros((self.batch_size, self.n_views, data.shape[2], data.shape[3]))\n",
        "    #     tmp_data = data[begin:end, :, :, :]\n",
        "    #     if type(tmp_data) is not np.ndarray:\n",
        "    #         tmp_data = tmp_data.toarray()  # convert sparse matrices\n",
        "        \n",
        "    #     batch_data = tmp_data\n",
        "    #     feed_dict2 = {self.ph_data2: batch_data, self.ph_dropout2: 1}\n",
        "    #     output_x = sess.run(self.feature_all, feed_dict2)\n",
        "    #     output_x = np.array(output_x)\n",
        "    #     print(output_x.shape)\n",
        "\n",
        "\n",
        "    def predict(self, data, labels=None, sess=None):\n",
        "        loss = 0\n",
        "        size = data.shape[0]\n",
        "        N,V,M,F = data.shape\n",
        "        feature_size = np.array(self.F[0])\n",
        "        print(feature_size, self.F)\n",
        "        output_feature2 = np.zeros([N,V,M*feature_size])\n",
        "\n",
        "        predictions = np.empty(size)\n",
        "        sess = self._get_session(sess)\n",
        "        output_feature = []\n",
        "\n",
        "        for begin in range(0, size, self.batch_size):\n",
        "            end = begin + self.batch_size\n",
        "            end = min([end, size])\n",
        "            batch_data = np.zeros((self.batch_size, self.n_views, data.shape[2], data.shape[3]))\n",
        "            tmp_data = data[begin:end, :, :, :]\n",
        "\n",
        "            if type(tmp_data) is not np.ndarray:\n",
        "                tmp_data = tmp_data.toarray()  # convert sparse matrices\n",
        "            batch_data[:end-begin] = tmp_data\n",
        "            feed_dict = {self.ph_data: batch_data, self.ph_dropout: 1}\n",
        "            # output_x = sess.run(self.feature_all, feed_dict)\n",
        "            # output_feature2[begin:end,:,:] = output_x[0:end-begin,:,:]\n",
        "\n",
        "            # Compute loss if labels are given.\n",
        "            if labels is not None:\n",
        "                batch_labels = np.zeros(self.batch_size)\n",
        "                batch_labels[:end-begin] = labels[begin:end]\n",
        "                feed_dict[self.ph_labels] = batch_labels\n",
        "                batch_pred, batch_loss = sess.run([self.op_prediction, self.op_loss], feed_dict)\n",
        "                loss += batch_loss\n",
        "            else:\n",
        "                batch_pred = sess.run(self.op_prediction, feed_dict)\n",
        "\n",
        "            predictions[begin:end] = batch_pred[:end-begin]\n",
        "        # print(output_feature2.shape)\n",
        "\n",
        "        if labels is not None:\n",
        "            return predictions, loss * self.batch_size / size, output_feature2\n",
        "        else:\n",
        "            return predictions\n",
        "\n",
        "\n",
        "    def evaluate(self, data, labels, sess=None):\n",
        "        \"\"\"\n",
        "        Runs one evaluation against the full epoch of data.\n",
        "        Return the precision and the number of correct predictions.\n",
        "        Batch evaluation saves memory and enables this to run on smaller GPUs.\n",
        "\n",
        "        sess: the session in which the model has been trained.\n",
        "        op: the Tensor that returns the number of correct predictions.\n",
        "        data: size N x M\n",
        "            N: number of signals (samples)\n",
        "            M: number of vertices (features)\n",
        "        labels: size N\n",
        "            N: number of signals (samples)\n",
        "        \"\"\"\n",
        "        t_process, t_wall = time.process_time(), time.time()\n",
        "        predictions, loss, output_feature_all = self.predict(data, labels, sess)\n",
        "\n",
        "        error = predictions-labels\n",
        "        error = np.abs(error)\n",
        "        error_sum = np.sum(error)\n",
        "        accuracy = 1 - error_sum/error.size\n",
        "        fpr, tpr, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "        auc = 100 * sklearn.metrics.auc(fpr, tpr)\n",
        "        string = 'auc: {:.2f}, loss: {:.2e}'.format(auc, loss)\n",
        "        print('accuracy = ', accuracy)\n",
        "        if sess is None:\n",
        "            string += '\\ntime: {:.0f}s (wall {:.0f}s)'.format(time.process_time()-t_process, time.time()-t_wall)\n",
        "        # return string, accuracy, auc, loss, predictions\n",
        "        return string, auc, loss, predictions, accuracy, output_feature_all\n",
        "\n",
        "\n",
        "    def fit(self, data, train_labels, val_data, val_labels):\n",
        "        t_process, t_wall = time.process_time(), time.time()\n",
        "        sess = tf.Session(graph=self.graph)\n",
        "        shutil.rmtree(self._get_path('summaries'), ignore_errors=True)\n",
        "        writer = tf.summary.FileWriter(self._get_path('summaries'), self.graph)\n",
        "        shutil.rmtree(self._get_path('checkpoints'), ignore_errors=True)\n",
        "        os.makedirs(self._get_path('checkpoints'))\n",
        "        path = os.path.join(self._get_path('checkpoints'), 'model')\n",
        "        sess.run(self.op_init)\n",
        "\n",
        "        # Training.\n",
        "        count = 0\n",
        "        bad_counter = 0\n",
        "        accuracies = []\n",
        "        aucs = []\n",
        "        losses = []\n",
        "        indices = collections.deque()\n",
        "        train_data_size = len(list(train_labels))\n",
        "        num_steps = int(self.num_epochs * train_data_size / self.batch_size)\n",
        "        estop = False  # early stop\n",
        "        n, v, m, f = data.shape\n",
        "        for step in range(1, num_steps+1):\n",
        "\n",
        "            # Be sure to have used all the samples before using one a second time.\n",
        "            if len(indices) < self.batch_size:\n",
        "                indices.extend(np.random.permutation(train_data_size))\n",
        "                # indices.extend(np.array(range(train_data_size)))\n",
        "            idx = [indices.popleft() for i in range(self.batch_size)]\n",
        "            count += len(idx)\n",
        "\n",
        "            train_data = data[idx, :, :, :]\n",
        "            batch_data, batch_labels = train_data, train_labels[idx]\n",
        "            if type(batch_data) is not np.ndarray:\n",
        "                batch_data = batch_data.toarray()  # convert sparse matrices\n",
        "            feed_dict = {self.ph_data: batch_data, self.ph_labels: batch_labels, self.ph_dropout: self.dropout}\n",
        "            learning_rate, loss_average = sess.run([self.op_train, self.op_loss_average], feed_dict)\n",
        "\n",
        "            # Periodical evaluation of the model.\n",
        "            if step % self.eval_frequency == 0 or step == num_steps:\n",
        "                # print ('Seen samples: %d' % count)\n",
        "                epoch = step * self.batch_size / train_data_size\n",
        "                # print('step {} / {} (epoch {:.2f} / {}):'.format(step, num_steps, epoch, self.num_epochs))\n",
        "                # print('  learning_rate = {:.2e}, loss_average = {:.2e}'.format(learning_rate, loss_average))\n",
        "                # string, auc, loss, predictions, accuracy = self.evaluate(val_data, val_labels, sess)\n",
        "                # aucs.append(auc)\n",
        "                # losses.append(loss)\n",
        "                # print('  validation {}'.format(string))\n",
        "                # print(predictions.tolist()[:50])\n",
        "                # print('  time: {:.0f}s (wall {:.0f}s)'.format(time.process_time()-t_process, time.time()-t_wall))\n",
        "\n",
        "                ## Summaries for TensorBoard.\n",
        "                summary = tf.Summary()\n",
        "                writer.add_summary(summary, step)\n",
        "\n",
        "                # Save model parameters (for evaluation).\n",
        "                self.op_saver.save(sess, path, global_step=step)\n",
        "\n",
        "                # if len(aucs) > (self.patience+5) and auc > np.array(aucs).max():\n",
        "                #     bad_counter = 0\n",
        "\n",
        "                # if len(aucs) > (self.patience+5) and auc <= np.array(aucs)[:-self.patience].max():\n",
        "                #     bad_counter += 1\n",
        "                #     if bad_counter > self.patience:\n",
        "                #         print('Early Stop!')\n",
        "                #         estop = True\n",
        "                #         break\n",
        "\n",
        "            if estop:\n",
        "                break\n",
        "        sess.close()\n",
        "\n",
        "        t_step = (time.time() - t_wall) / num_steps\n",
        "        return aucs, losses, t_step\n",
        "\n",
        "    def get_var(self, name):\n",
        "        sess = self._get_session()\n",
        "        var = self.graph.get_tensor_by_name(name + ':0')\n",
        "        val = sess.run(var)\n",
        "        sess.close()\n",
        "        return val\n",
        "\n",
        "    # Methods to construct the computational graph.\n",
        "    def build_multi_gcn_graph(self, M_0):\n",
        "        \"\"\"Build the computational graph of the model.\"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "\n",
        "            # Inputs.\n",
        "            with tf.name_scope('inputs'):\n",
        "                self.ph_data = tf.placeholder(tf.float32,(self.batch_size, self.n_views, M_0, self.fin), 'data')\n",
        "                # self.ph_data = tf.placeholder(tf.float32,(None, self.n_views, M_0, self.fin), 'data')\n",
        "                self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n",
        "                self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
        "            # Model.\n",
        "            op_logits, feature_all = self.inference(self.ph_data, self.ph_dropout)\n",
        "\n",
        "            self.my_M0 = M_0\n",
        "            self.feature_all = feature_all\n",
        "\n",
        "            self.op_loss, self.op_loss_average = self.loss(op_logits, self.ph_labels, self.regularization)\n",
        "            self.op_train = self.training(self.op_loss, self.learning_rate,\n",
        "                    self.decay_steps, self.decay_rate, self.momentum)\n",
        "            self.op_prediction = self.prediction(op_logits)\n",
        "\n",
        "\n",
        "            # Initialize variables, i.e. weights and biases.\n",
        "            self.op_init = tf.global_variables_initializer()\n",
        "\n",
        "            # Summaries for TensorBoard and Save for model parameters.\n",
        "            self.op_summary = tf.summary.merge_all()\n",
        "            self.op_saver = tf.train.Saver(max_to_keep=5)\n",
        "\n",
        "        self.graph.finalize()\n",
        "\n",
        "    # Methods to construct the computational graph.\n",
        "    def build_gcn_graph(self, M_0):\n",
        "        \"\"\"Build the computational graph of the model.\"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "\n",
        "            # Inputs.\n",
        "            with tf.name_scope('inputs'):\n",
        "                self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0, self.fin), 'data')\n",
        "                self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n",
        "                self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
        "\n",
        "            # Model.\n",
        "            op_logits = self.inference(self.ph_data, self.ph_dropout)\n",
        "            self.op_loss, self.op_loss_average = self.loss(op_logits, self.ph_labels, self.regularization)\n",
        "            self.op_train = self.training(self.op_loss, self.learning_rate,\n",
        "                    self.decay_steps, self.decay_rate, self.momentum)\n",
        "            self.op_prediction = self.prediction(op_logits)\n",
        "\n",
        "            # Initialize variables, i.e. weights and biases.\n",
        "            self.op_init = tf.global_variables_initializer()\n",
        "\n",
        "            # Summaries for TensorBoard and Save for model parameters.\n",
        "            self.op_summary = tf.summary.merge_all()\n",
        "            self.op_saver = tf.train.Saver(max_to_keep=5)\n",
        "\n",
        "        self.graph.finalize()\n",
        "\n",
        "\n",
        "    def inference(self, data, dropout):\n",
        "        \"\"\"\n",
        "        It builds the model, i.e. the computational graph, as far as\n",
        "        is required for running the network forward to make predictions,\n",
        "        i.e. return logits given raw data.\n",
        "\n",
        "        data: size N x M\n",
        "            N: number of signals (samples)\n",
        "            M: number of vertices (features)\n",
        "        training: we may want to discriminate the two, e.g. for dropout.\n",
        "            True: the model is built for training.\n",
        "            False: the model is built for evaluation.\n",
        "        \"\"\"\n",
        "        # TODO: optimizations for sparse data\n",
        "        logits, feature_all = self._inference(data, dropout)\n",
        "        return logits, feature_all\n",
        "\n",
        "\n",
        "    def probabilities(self, logits):\n",
        "        \"\"\"Return the probability of a sample to belong to each class.\"\"\"\n",
        "        with tf.name_scope('probabilities'):\n",
        "            probabilities = tf.nn.softmax(logits)\n",
        "            return probabilities\n",
        "\n",
        "    def prediction(self, logits):\n",
        "        \"\"\"Return the predicted classes.\"\"\"\n",
        "        with tf.name_scope('prediction'):\n",
        "            prediction = tf.argmax(logits, axis=1)\n",
        "            # prediction = logits\n",
        "            return prediction\n",
        "\n",
        "    def loss(self, logits, labels, regularization):\n",
        "        \"\"\"Adds to the inference model the layers required to generate loss.\"\"\"\n",
        "        with tf.name_scope('loss'):\n",
        "            with tf.name_scope('cross_entropy'):\n",
        "                labels = tf.to_int64(labels)\n",
        "                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "                cross_entropy = tf.reduce_mean(cross_entropy)\n",
        "            with tf.name_scope('regularization'):\n",
        "                regularization *= tf.add_n(self.regularizers)\n",
        "            loss = cross_entropy + regularization\n",
        "\n",
        "            # Summaries for TensorBoard.\n",
        "            tf.summary.scalar('loss/cross_entropy', cross_entropy)\n",
        "            tf.summary.scalar('loss/regularization', regularization)\n",
        "            tf.summary.scalar('loss/total', loss)\n",
        "            with tf.name_scope('averages'):\n",
        "                averages = tf.train.ExponentialMovingAverage(0.9)\n",
        "                op_averages = averages.apply([cross_entropy, regularization, loss])\n",
        "                tf.summary.scalar('loss/avg/cross_entropy', averages.average(cross_entropy))\n",
        "                tf.summary.scalar('loss/avg/regularization', averages.average(regularization))\n",
        "                tf.summary.scalar('loss/avg/total', averages.average(loss))\n",
        "                with tf.control_dependencies([op_averages]):\n",
        "                    loss_average = tf.identity(averages.average(loss), name='control')\n",
        "            return loss, loss_average\n",
        "\n",
        "\n",
        "    def training(self, loss, learning_rate, decay_steps, decay_rate=0.95, momentum=0.9):\n",
        "        \"\"\"Adds to the loss model the Ops required to generate and apply gradients.\"\"\"\n",
        "        with tf.name_scope('training'):\n",
        "            # Learning rate.\n",
        "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "            if decay_rate != 1:\n",
        "                learning_rate = tf.train.exponential_decay(\n",
        "                        learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
        "            tf.summary.scalar('learning_rate', learning_rate)\n",
        "            # Optimizer.\n",
        "            if momentum == 0:\n",
        "                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "                #optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "            else:\n",
        "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "            grads = optimizer.compute_gradients(loss)\n",
        "            op_gradients = optimizer.apply_gradients(grads, global_step=global_step)\n",
        "            # Histograms.\n",
        "            for grad, var in grads:\n",
        "                if grad is None:\n",
        "                    print('warning: {} has no gradient'.format(var.op.name))\n",
        "                else:\n",
        "                    tf.summary.histogram(var.op.name + '/gradients', grad)\n",
        "            # The op return the learning rate.\n",
        "            with tf.control_dependencies([op_gradients]):\n",
        "                op_train = tf.identity(learning_rate, name='control')\n",
        "            return op_train\n",
        "\n",
        "    # Helper methods.\n",
        "\n",
        "    def _get_path(self, folder):\n",
        "        # path = '../../models/'\n",
        "        path = 'models/'\n",
        "        return os.path.join(path, folder, self.dir_name)\n",
        "\n",
        "\n",
        "    def _get_session(self, sess=None):\n",
        "        \"\"\"Restore parameters if no session given.\"\"\"\n",
        "        if sess is None:\n",
        "            sess = tf.Session(graph=self.graph)\n",
        "            filename = tf.train.latest_checkpoint(self._get_path('checkpoints'))\n",
        "            self.op_saver.restore(sess, filename)\n",
        "        return sess\n",
        "\n",
        "    def _weight_variable(self, shape, regularization=True):\n",
        "        initial = tf.truncated_normal_initializer(0, 0.1)\n",
        "        var = tf.get_variable('weights', shape, tf.float32, initializer=initial)\n",
        "        if regularization:\n",
        "          self.regularizers.append(tf.nn.l2_loss(var))\n",
        "        tf.summary.histogram(var.op.name, var)\n",
        "        return var\n",
        "\n",
        "    def _bias_variable(self, shape, regularization=True):\n",
        "        initial = tf.constant_initializer(0.1)\n",
        "        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n",
        "        if regularization:\n",
        "            self.regularizers.append(tf.nn.l2_loss(var))\n",
        "        tf.summary.histogram(var.op.name, var)\n",
        "        return var\n",
        "\n",
        "    def _conv2d(self, x, W):\n",
        "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "\n",
        "class siamese_m_cgcnn(base_model):\n",
        "    \"\"\"\n",
        "    Graph CNN which uses the Chebyshev approximation.\n",
        "\n",
        "    The following are hyper-parameters of graph convolutional layers.\n",
        "    They are lists, which length is equal to the number of gconv layers.\n",
        "        F: Number of features.\n",
        "        K: List of polynomial orders, i.e. filter sizes or number of hopes.\n",
        "        p: Pooling size.\n",
        "           Should be 1 (no pooling) or a power of 2 (reduction by 2 at each coarser level).\n",
        "           Beware to have coarsened enough.\n",
        "\n",
        "    L: List of Graph Laplacians. Size M x M. One per coarsening level.\n",
        "\n",
        "    The following are hyper-parameters of fully connected layers.\n",
        "    They are lists, which length is equal to the number of fc layers.\n",
        "        M: Number of features per sample, i.e. number of hidden neurons.\n",
        "           The last layer is the softmax, i.e. M[-1] is the number of classes.\n",
        "\n",
        "    The following are choices of implementation for various blocks.\n",
        "        filter: filtering operation, e.g. chebyshev5, lanczos2 etc.\n",
        "        brelu: bias and relu, e.g. b1relu or b2relu.\n",
        "        pool: pooling, e.g. mpool1.\n",
        "\n",
        "    Training parameters:\n",
        "        num_epochs:    Number of training epochs.\n",
        "        learning_rate: Initial learning rate.\n",
        "        decay_rate:    Base of exponential decay. No decay with 1.\n",
        "        decay_steps:   Number of steps after which the learning rate decays.\n",
        "        momentum:      Momentum. 0 indicates no momentum.\n",
        "\n",
        "    Regularization parameters:\n",
        "        regularization: L2 regularizations of weights and biases.\n",
        "        dropout:        Dropout (fc layers): probability to keep hidden neurons. No dropout with 1.\n",
        "        batch_size:     Batch size. Must divide evenly into the dataset sizes.\n",
        "        eval_frequency: Number of steps between evaluations.\n",
        "\n",
        "    Directories:\n",
        "        dir_name: Name for directories (summaries and model parameters).\n",
        "    \"\"\"\n",
        "    def __init__(self, L, F, K, p, M, fin, n_views, view_com, method='gcn', filter='chebyshev5', brelu='b1relu', pool='mpool1',\n",
        "                num_epochs=20, learning_rate=0.01, decay_rate=0.95, decay_steps=None, momentum=0.9,\n",
        "                regularization=0, dropout=0, batch_size=100, eval_frequency=200, patience=10,\n",
        "                dir_name=''):\n",
        "        super().__init__()\n",
        "\n",
        "        # Verify the consistency w.r.t. the number of layers.\n",
        "        assert len(L) >= len(F) == len(K) == len(p)\n",
        "        assert np.all(np.array(p) >= 1)\n",
        "        p_log2 = np.where(np.array(p) > 1, np.log2(p), 0)\n",
        "        assert np.all(np.mod(p_log2, 1) == 0)  # Powers of 2.\n",
        "        assert len(L) >= 1 + np.sum(p_log2)  # Enough coarsening levels for pool sizes.\n",
        "\n",
        "        # Keep the useful Laplacians only. May be zero.\n",
        "        M_0 = L[0].shape[0]\n",
        "        j = 0\n",
        "        self.L = []\n",
        "        for pp in p:\n",
        "            self.L.append(L[j])\n",
        "            j += int(np.log2(pp)) if pp > 1 else 0\n",
        "        L = self.L\n",
        "\n",
        "        # Print information about NN architecture.\n",
        "        Ngconv = len(p)\n",
        "        Nfc = len(M)\n",
        "        # print('NN architecture')\n",
        "        # print('  input: M_0 = {}'.format(M_0))\n",
        "        for i in range(Ngconv):\n",
        "            # print('  layer {0}: cgconv{0}'.format(i+1))\n",
        "            # print('    representation: M_{0} * F_{1} / p_{1} = {2} * {3} / {4} = {5}'.format(\n",
        "            #         i, i+1, L[i].shape[0], F[i], p[i], L[i].shape[0]*F[i]*fin//p[i]))\n",
        "            F_last = F[i-1] if i > 0 else 1\n",
        "            # print('    weights: F_{0} * F_{1} * K_{1} = {2} * {3} * {4} = {5}'.format(\n",
        "            #         i, i+1, F_last, F[i], K[i], F_last*F[i]*K[i]))\n",
        "            # if brelu == 'b1relu':\n",
        "                # print('    biases: F_{} = {}'.format(i+1, F[i]))\n",
        "            # elif brelu == 'b2relu':\n",
        "                # print('    biases: M_{0} * F_{0} = {1} * {2} = {3}'.format(\n",
        "                #         i+1, L[i].shape[0], F[i], L[i].shape[0]*F[i]))\n",
        "        for i in range(Nfc):\n",
        "            name = 'logits (softmax)' if i == Nfc-1 else 'fc{}'.format(i+1)\n",
        "            # print('  layer {}: {}'.format(Ngconv+i+1, name))\n",
        "            # print('    representation: M_{} = {}'.format(Ngconv+i+1, M[i]))\n",
        "            M_last = M[i-1] if i > 0 else M_0 if Ngconv == 0 else L[-1].shape[0] * F[-1] // p[-1]\n",
        "            # print('    weights: M_{} * M_{} = {} * {} = {}'.format(\n",
        "            #         Ngconv+i, Ngconv+i+1, M_last, M[i], M_last*M[i]))\n",
        "            # print('    biases: M_{} = {}'.format(Ngconv+i+1, M[i]))\n",
        "\n",
        "        # Store attributes and bind operations.\n",
        "        self.n_views, self.view_com = n_views, view_com\n",
        "        self.data_size = 7\n",
        "        self.L, self.F, self.K, self.p, self.M, self.fin = L, F, K, p, M, fin\n",
        "        self.num_epochs, self.learning_rate, self.patience = num_epochs, learning_rate, patience\n",
        "        self.decay_rate, self.decay_steps, self.momentum = decay_rate, decay_steps, momentum\n",
        "        self.regularization, self.dropout = regularization, dropout\n",
        "        self.batch_size, self.eval_frequency = batch_size, eval_frequency\n",
        "        self.dir_name = dir_name\n",
        "        self.method = method\n",
        "        self.filter = getattr(self, filter)\n",
        "        self.brelu = getattr(self, brelu)\n",
        "        self.pool = getattr(self, pool)\n",
        "        # Build the computational graph.\n",
        "        self.build_multi_gcn_graph(M_0)\n",
        "\n",
        "\n",
        "    def chebyshev5(self, x, L, Fout, K):\n",
        "        if(self.batch_size == 1):\n",
        "          x = x[np.newaxis]\n",
        "        N, M, Fin = x.get_shape()\n",
        "        N, M, Fin = int(N), int(M), int(Fin)\n",
        "        # Rescale Laplacian and store as a TF sparse tensor. Copy to not modify the shared L.\n",
        "        L = scipy.sparse.csr_matrix(L)\n",
        "        L = rescale_L(L, 2)\n",
        "        L = L.tocoo()\n",
        "        indices = np.column_stack((L.row, L.col))\n",
        "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
        "        L = tf.sparse_reorder(L)\n",
        "        # Transform to Chebyshev basis\n",
        "        x0 = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
        "        x0 = tf.reshape(x0, [M, Fin*N])  # M x Fin*N\n",
        "        x = tf.expand_dims(x0, 0)  # 1 x M x Fin*N\n",
        "        def concat(x, x_):\n",
        "            x_ = tf.expand_dims(x_, 0)  # 1 x M x Fin*N\n",
        "            return tf.concat([x, x_], axis=0)  # K x M x Fin*N\n",
        "        if K > 1:\n",
        "            x1 = tf.sparse_tensor_dense_matmul(L, x0)\n",
        "            x = concat(x, x1)\n",
        "        for k in range(2, K):\n",
        "            x2 = 2 * tf.sparse_tensor_dense_matmul(L, x1) - x0  # M x Fin*N\n",
        "            x = concat(x, x2)\n",
        "            x0, x1 = x1, x2\n",
        "        x = tf.reshape(x, [K, M, Fin, N])  # K x M x Fin x N\n",
        "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
        "        x = tf.reshape(x, [N*M, Fin*K])  # N*M x Fin*K\n",
        "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature pair.\n",
        "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
        "        x = tf.matmul(x, W)  # N*M x Fout\n",
        "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
        "\n",
        "    def b1relu(self, x):\n",
        "        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n",
        "        N, M, F = x.get_shape()\n",
        "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
        "        return tf.nn.relu(x + b)\n",
        "\n",
        "    def b2relu(self, x):\n",
        "        \"\"\"Bias and ReLU. One bias per vertex per filter.\"\"\"\n",
        "        N, M, F = x.get_shape()\n",
        "        b = self._bias_variable([1, int(M), int(F)], regularization=False)\n",
        "        return tf.nn.relu(x + b)\n",
        "\n",
        "    def mpool1(self, x, p):\n",
        "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
        "        if p > 1:\n",
        "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
        "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
        "            #tf.maximum\n",
        "            return tf.squeeze(x, [3])  # N x M/p x F\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def apool1(self, x, p):\n",
        "        \"\"\"Average pooling of size p. Should be a power of 2.\"\"\"\n",
        "        if p > 1:\n",
        "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
        "            x = tf.nn.avg_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
        "            return tf.squeeze(x, [3])  # N x M/p x F\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def fc(self, x, Mout, relu=False):\n",
        "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
        "        N, Min = x.get_shape()\n",
        "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
        "        b = self._bias_variable([Mout], regularization=True)\n",
        "        x = tf.matmul(x, W) + b\n",
        "        return tf.nn.relu(x) if relu else x\n",
        "        \n",
        "        \n",
        "    def _inference_single(self, x, dropout, name, reuse=False):\n",
        "        x_0 = tf.squeeze(x)\n",
        "\n",
        "        with tf.variable_scope(\"siamese\", reuse=reuse) as scope:\n",
        "            for i in range(len(self.p)):\n",
        "                with tf.variable_scope('conv1{}'.format(i+1)):\n",
        "                    with tf.name_scope('filter'):\n",
        "                        x_0 = self.filter(x_0, self.L[i], self.F[i], self.K[i])\n",
        "                    with tf.name_scope('bias_relu'):\n",
        "                        x_0 = self.brelu(x_0)\n",
        "                    with tf.name_scope('pooling'):\n",
        "                        x_0 = self.pool(x_0, self.p[i])\n",
        "\n",
        "        return x_0\n",
        "\n",
        "    def _view_pool(self, view_features, name, method='max'):\n",
        "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
        "\n",
        "        vp = tf.expand_dims(view_features[0], 0) # eg. [100] -> [1, 100]\n",
        "        for v in view_features[1:]:\n",
        "            v = tf.expand_dims(v, 0)\n",
        "            vp = tf.concat([vp, v], axis=0)\n",
        "        # print ('vp before reducing:', vp.get_shape().as_list())\n",
        "        if method == 'max':\n",
        "            vp = tf.reduce_max(vp, [0], name=name)\n",
        "        elif method == 'mean':\n",
        "            vp = tf.reduce_mean(vp, [0], name=name)\n",
        "        return vp   \n",
        "\n",
        "    def _inference(self, views, dropout):\n",
        "        \"\"\"views: N x V x M x F tensor\"\"\"\n",
        "\n",
        "        n_views = views.get_shape().as_list()[1]\n",
        "        _,V,_,_ = views.get_shape()\n",
        "        # transpose views : (NxVxMxF) -> (VxNxMxF)\n",
        "        views = tf.transpose(views, perm=[1, 0, 2, 3])\n",
        "\n",
        "        view_pool_0 = []\n",
        "        for i in range(n_views):\n",
        "\n",
        "            # set reuse True for i > 0, for weight-sharing\n",
        "            reuse = (i != 0)\n",
        "            # reuse = False\n",
        "            view = tf.gather(views, i) # NxMxF\n",
        "\n",
        "            x_0 = self._inference_single(view, dropout, i, reuse)\n",
        "            # x_0 = tf.nn.l2_normalize(x_0, dim=2, epsilon=1e-12, name=None)\n",
        "            N, M, F = x_0.get_shape()\n",
        "            x_0 = tf.reshape(x_0, [int(N), int(M * F)])\n",
        "            view_pool_0.append(x_0)\n",
        "\n",
        "        # view_pool_0 = tf.reshape(view_pool_0,[int(N), int(V)])\n",
        "        feature_all = tf.reshape(view_pool_0,[int(N), int(V), int(M * F)])\n",
        "        # max pooling for views\n",
        "        pool_vp_0 = self._view_pool(view_pool_0, 'pool_vp', self.view_com)\n",
        "        # print ('pool_vp_0', pool_vp_0.get_shape().as_list())\n",
        "\n",
        "        # Dot product layer\n",
        "        x_0 = tf.reshape(pool_vp_0, [int(N * M), int(F)])\n",
        "        x_0 = tf.nn.l2_normalize(x_0, dim=1, epsilon=1e-12, name=None)\n",
        "\n",
        "        # x_ = tf.reduce_sum(x_, 1, keep_dims=True)\n",
        "        x_ = tf.reshape(x_0, [int(N), int(M), int(F)])\n",
        "    \n",
        "        # Fully connected hidden layers.\n",
        "        N, M, F = x_.get_shape()\n",
        "        # print(N,M,F)\n",
        "        x_ = tf.reshape(x_, [int(N), int(M*F)])  # N x M x F\n",
        "        x1 = x_\n",
        "        for i, M in enumerate(self.M[:-1]):\n",
        "            with tf.variable_scope('fc{}'.format(i+1)):\n",
        "                x_ = self.fc(x_, M)\n",
        "                x_ = tf.nn.dropout(x_, dropout)\n",
        "\n",
        "        # Logits linear layer, i.e. softmax without normalization.\n",
        "        with tf.variable_scope('logits'):\n",
        "            x_ = self.fc(x_, self.M[-1], relu=False)\n",
        "\n",
        "        return x_, feature_all  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fflgDEvB3RgV",
        "colab_type": "text"
      },
      "source": [
        "### Define Training (Train.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oruPsCKkuplR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import sys, os\n",
        "sys.path.insert(0, '..')\n",
        "# import models, graph, coarsening, utils\n",
        "# from utils import model_perf\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import pickle as pkl\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# %matplotlib inline\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# neural network setting\n",
        "# Graphs.\n",
        "# flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
        "# flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n",
        "# TODO: change cgcnn for combinatorial Laplacians.\n",
        "# flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
        "# flags.DEFINE_integer('coarsening_levels', 4, 'Number of coarsened graphs.')\n",
        "\n",
        "results_auc = dict()\n",
        "results = list()\n",
        "\n",
        "class model_perf(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.names, self.params = set(), {}\n",
        "        self.fit_auc, self.fit_losses, self.fit_time = {}, {}, {}\n",
        "        self.train_auc, self.train_loss = {}, {}\n",
        "        self.test_auc, self.test_loss = {}, {}\n",
        "        self.train_represent = {}\n",
        "\n",
        "\n",
        "    def test(self, model, name, params, data, train_labels, val_data, val_labels, test_chosen_index, test_labels, mode, label_all):\n",
        "        self.params[name] = params\n",
        "        self.fit_auc[name], self.fit_losses[name], self.fit_time[name] = model.fit(data, train_labels, val_data, val_labels)\n",
        "\n",
        "        # del val_data, val_labels\n",
        "        n, v, m, f = data.shape\n",
        "        if params['method'] == 'gcn' or params['method'] == '2gcn':\n",
        "            test_data = data[test_chosen_index[:], :, :, :]\n",
        "        elif params['method'] == 'fnn' or params['method'] == '2fnn':\n",
        "            new_data = np.zeros([n, v, m*f])\n",
        "            for i in range(n):\n",
        "                for j in range(v):\n",
        "                    new_data[i, j, :] = data[i, j, :, :].flatten()\n",
        "            test_data = new_data[test_chosen_index[:], :, :]\n",
        "\n",
        "        print('****** start_testing *******')\n",
        "        if(mode == 'validate'):\n",
        "          # string, self.test_auc[name], self.test_loss[name], predictions, accuracy,_ = model.evaluate(test_data, test_labels)\n",
        "          string, self.test_auc[name], self.test_loss[name], predictions, accuracy,_ = model.evaluate(val_data, val_labels)\n",
        "          # print(val_data.shape)\n",
        "        elif(mode == 'test'):\n",
        "          string, self.test_auc[name], self.test_loss[name], predictions, accuracy,_ = model.evaluate(test_data, test_labels)\n",
        "          print(predictions)\n",
        "          # string, self.test_auc[name], self.test_loss[name], predictions, accuracy, output_feature_all = model.evaluate(data, label_all)\n",
        "          # N1,V1,F1 = output_feature_all.shape\n",
        "          # output_feature_reshape = np.reshape(output_feature_all,[int(N1),int(V1*F1)])\n",
        "          # print('now we write')\n",
        "          # np.savetxt(r'results/HIV_feature_all_input_HOSVD_m_30.txt',output_feature_reshape,fmt=\"%f\")\n",
        "        # model.output_feature(test_data, test_labels)\n",
        "        # print('test  {}'.format(string))\n",
        "\n",
        "        # if(mode == 'test'):\n",
        "        #   test_labels = label_all\n",
        "\n",
        "        fpr, tpr, _ = sklearn.metrics.roc_curve(test_labels, predictions)\n",
        "        auc = 100 * sklearn.metrics.auc(fpr, tpr)\n",
        "        tn, fp, fn, tp = confusion_matrix(test_labels, predictions).ravel()\n",
        "        sensitivity = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp) \n",
        "\n",
        "\n",
        "        self.names.add(name)\n",
        "        return accuracy, auc, sensitivity, specificity\n",
        "        \n",
        "\n",
        "    def fin_result(self, data_type, i_fold=None):\n",
        "        for name in sorted(self.names):\n",
        "            if name not in results_auc:\n",
        "                results_auc[name] = 0\n",
        "            results_auc[name] += self.test_auc[name]\n",
        "            results.append([i_fold, self.test_auc[name]])\n",
        "        if i_fold == 4:\n",
        "            for name in sorted(self.names):\n",
        "                results_auc[name] /= 5\n",
        "                print('{:5.2f}  {}'.format(\n",
        "                    results_auc[name], name))\n",
        "            results.append([name, results_auc[name]])\n",
        "            # r = pd.DataFrame(data=results)\n",
        "            # r.to_csv('../../../data/output/' + data_type + '_fin_results', index=False, header=['method', 'test_auc'])\n",
        "\n",
        "\n",
        "    def show(self, fontsize=None):\n",
        "        if fontsize:\n",
        "            plt.rc('pdf', fonttype=42)\n",
        "            plt.rc('ps', fonttype=42)\n",
        "            plt.rc('font', size=fontsize)         # controls default text sizes\n",
        "            plt.rc('axes', titlesize=fontsize)    # fontsize of the axes title\n",
        "            plt.rc('axes', labelsize=fontsize)    # fontsize of the x any y labels\n",
        "            plt.rc('xtick', labelsize=fontsize)   # fontsize of the tick labels\n",
        "            plt.rc('ytick', labelsize=fontsize)   # fontsize of the tick labels\n",
        "            plt.rc('legend', fontsize=fontsize)   # legend fontsize\n",
        "            plt.rc('figure', titlesize=fontsize)  # size of the figure title\n",
        "        # print('  auc      loss        time [ms]  name')\n",
        "        # print('test  train   test  train   test     train')\n",
        "        # for name in sorted(self.names):\n",
        "        #     print('{:5.2f} {:5.2f}   {:.2e} {:.2e}   {:3.0f}   {}'.format(\n",
        "        #             self.test_auc[name], self.train_auc[name],\n",
        "        #             self.test_loss[name], self.train_loss[name], self.fit_time[name]*1000, name))\n",
        "\n",
        "\n",
        "\n",
        "def get_feed_data(data, subj, indexes, labels, method='gcn'):\n",
        "    train_indexes, val_indexes, test_indexes = indexes\n",
        "    train_labels, val_labels, test_labels = labels\n",
        "    n, v, m, f = data.shape\n",
        "    # if v == 6:\n",
        "    #     print (val_labels.shape)\n",
        "    #     n_val_indexes = 10000\n",
        "    #     sidx = np.random.permutation(val_labels.shape[0])\n",
        "    #     val_indexes =  np.array([val_indexes[s] for s in sidx[:n_val_indexes]])\n",
        "    #     val_labels = np.array([val_labels[s] for s in sidx[:n_val_indexes]])\n",
        "    # f = 1 # whether f can be deleted\n",
        "    if method == 'gcn' or method == '2gcn':\n",
        "        val_x = data[val_indexes[:], :, :, :]\n",
        "    elif method == 'fnn' or method == '2fnn':\n",
        "        new_data = np.zeros([n, v, m*f])\n",
        "        for i in range(n):\n",
        "            for j in range(v):\n",
        "                new_data[i, j, :] = data[i, j, :, :].flatten()\n",
        "        val_x = new_data[val_indexes[:], :, :]\n",
        "\n",
        "    train_y = train_labels\n",
        "    val_y = val_labels\n",
        "    test_y = test_labels\n",
        "    del subj\n",
        "    del train_labels, val_labels, test_labels\n",
        "    del val_indexes\n",
        "    return train_indexes, train_y, val_x, val_y, test_indexes, test_y\n",
        "\n",
        "\n",
        "def train(method, view_com, n_views, k, theta, m, n_epoch, batch_size, pairs, labels, node_features, subj, data, data_type, i_fold, mode, label_all):\n",
        "    str_params = view_com + '_k' + str(k) + '_m' + str(m) + '_'\n",
        "    obj_params = 'softmax'\n",
        "\n",
        "    # print ('Construct ROI graphs...')\n",
        "    t_start = time.process_time()\n",
        "\n",
        "    dist, idx = compute_dist(node_features, k, metric = 'euclidean')\n",
        "    A = build_ajacency(dist, idx).astype(np.float32)\n",
        "\n",
        "    if method == '2gcn':\n",
        "        graphs, perm = coarsen(A, levels=FLAGS.coarsening_levels, self_connections=False)\n",
        "        # graphs, perm = coarsen(A, levels = 6, self_connections=False)\n",
        "        L = [build_laplacian(A, normalized=True) for A in graphs]\n",
        "        data = perm_data1(data, perm, True)\n",
        "    else:\n",
        "        graphs = list()\n",
        "        graphs.append(A)\n",
        "        L = [build_laplacian(A, normalized=True)]\n",
        "        \n",
        "\n",
        "    # print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
        "    # graph.plot_spectrum(L)\n",
        "    del A\n",
        "\n",
        "    print ('Set parameters...')\n",
        "    mp = model_perf()\n",
        "    # Architecture.\n",
        "    common = {}\n",
        "    # common['dir_name']       = 'bp/'\n",
        "    common['dir_name']       = 'HIV/'\n",
        "    common['num_epochs']     = n_epoch\n",
        "    common['batch_size']     = batch_size\n",
        "    common['eval_frequency'] = 5 * common['num_epochs']\n",
        "    common['patience']       = 5\n",
        "    common['regularization'] = 5e-3\n",
        "    common['dropout']        = 0\n",
        "    common['learning_rate']  = 1e-2\n",
        "    common['decay_rate']     = 0.95\n",
        "    common['momentum']       = 0.9\n",
        "    common['n_views']        = n_views\n",
        "    common['view_com']       = view_com\n",
        "\n",
        "    print ('Get feed indexes and labels...')\n",
        "    train_indexes, train_y, val_x, val_y, test_indexes, test_y = get_feed_data(data, subj, pairs, labels, method)\n",
        "    # print(train_indexes)\n",
        "    # print(val_x)\n",
        "    # print(test_indexes)\n",
        "\n",
        "    C = max(train_y)+1\n",
        "    common['decay_steps']    = train_indexes.shape[0] / (common['batch_size'] * 5)\n",
        "\n",
        "\n",
        "    if method == 'gcn':\n",
        "        # str_params += 'b_max_eu_'\n",
        "        name = 'mvgcn'\n",
        "        params = common.copy()\n",
        "        params['method'] = 'gcn'\n",
        "        params['F']              = [m] # filters\n",
        "        params['K']              = [theta] # supports\n",
        "        params['p']              = [1]\n",
        "        params['M']              = [C]\n",
        "        params['fin'] = val_x.shape[3]\n",
        "        params['dir_name'] += name\n",
        "        params['filter'] = 'chebyshev5'\n",
        "        params['brelu'] = 'b2relu'\n",
        "        params['pool'] = 'apool1'\n",
        "        accuracy, auc, sensitivity, specificity = mp.test(siamese_m_cgcnn(L, **params), name, params,\n",
        "                        data, train_y, val_x, val_y, test_indexes, test_y, mode, label_all)\n",
        "        return accuracy, auc, sensitivity, specificity\n",
        "\n",
        "\n",
        "    # mp.save(data_type)\n",
        "    method_type = method + '_'\n",
        "    # mp.fin_result(method_type + data_type + str_params + obj_params, i_fold)\n",
        "    mp.fin_result(method_type + 'str' + str_params + obj_params, i_fold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jim0O1YMbxre",
        "colab_type": "text"
      },
      "source": [
        "### Prepare for input parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9r85IAJHBCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn, sklearn.datasets\n",
        "import sklearn.naive_bayes, sklearn.linear_model, sklearn.svm, sklearn.neighbors, sklearn.ensemble\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse\n",
        "import numpy as np\n",
        "import time, re\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "from  scipy import *\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def convert_minus_one_2_zero(label):\n",
        "  for i in range(len(label)):\n",
        "    if(label[i]<0):\n",
        "      label[i] = 0;\n",
        "  return label    \n",
        "\n",
        "def load_data(f):\n",
        "  k = 0\n",
        "  input_data = {}\n",
        "\n",
        "  for line in f:\n",
        "    input_data[k] = np.int32(line.split())\n",
        "    k = k + 1\n",
        "  \n",
        "  return input_data\n",
        "\n",
        "\n",
        "def load_train_index(f):\n",
        "  train_index = []\n",
        "  k = 0\n",
        "  for line in f:\n",
        "    train_index.append(np.int32(line))\n",
        "\n",
        "  return train_index\n",
        "    \n",
        "\n",
        "\n",
        "def load_data_my_new(data_name, train_fold_chosen):\n",
        "  # load data from google drive\n",
        "  if(data_name == 'BP'):\n",
        "    x = scipy.io.loadmat('BP.mat')\n",
        "    X_normalize = x['X_normalize']\n",
        "    label = x['label']\n",
        "  elif(data_name == 'HIV'):\n",
        "    x = scipy.io.loadmat('HIV.mat') \n",
        "    X_normalize = x['X'] \n",
        "    label = x['label']\n",
        "  elif(data_name == 'PPMI'):\n",
        "    x = scipy.io.loadmat('PPMI.mat') \n",
        "    X_normalize = x['X'] \n",
        "    label = x['label']\n",
        "\n",
        "  # reshape data_size to N,V,M,F  \n",
        "  N_subject = X_normalize.size\n",
        "  X_1 = X_normalize[0][0]\n",
        "  M,F,V = X_1.shape\n",
        "  print('loading data:',data_name,'of size:',N_subject,V,M,F)\n",
        "  data = np.zeros([N_subject,V,M,F])\n",
        "  for i in range(N_subject):\n",
        "    X_i = X_normalize[i][0]\n",
        "    for j in range(V):\n",
        "      X_ij = X_i[:,:,j]\n",
        "      data[i,j,:,:] = X_ij\n",
        "\n",
        "  labels = np.array(label)\n",
        "  if(data_name == 'BP'):\n",
        "    f_test = open('results/BP_divide_test_index.txt','r')\n",
        "    test_idx = load_data(f_test)\n",
        "    f_train = open('results/BP_divide_train_index.txt','r')\n",
        "    train_idx = load_data(f_train)\n",
        "  elif(data_name == 'HIV'):\n",
        "    f_test = open('results/HIV_divide_test_index.txt','r')\n",
        "    test_idx = load_data(f_test)\n",
        "    f_train = open('results/HIV_divide_train_index.txt','r')\n",
        "    train_idx = load_data(f_train)\n",
        "\n",
        "  f_test.close()\n",
        "  f_train.close()\n",
        "\n",
        "  train_set_ratio = 0.8\n",
        "  train_chosen_idx = train_idx[train_fold_chosen]\n",
        "  data_size = train_chosen_idx.size;  \n",
        "  train_data_size = np.floor(data_size*train_set_ratio)\n",
        "  train_index = random.sample(list(train_chosen_idx), int(train_data_size))\n",
        "  test_index = np.setdiff1d(train_chosen_idx,train_index)\n",
        "\n",
        "  labels_set = list()\n",
        "  indexes_set = list()\n",
        "\n",
        "  train_label, test_label = labels[train_index], labels[test_index]\n",
        "  val_label,val_index = test_label,test_index\n",
        "  train_label = convert_minus_one_2_zero(train_label)\n",
        "  val_label = convert_minus_one_2_zero(val_label)\n",
        "  test_label = convert_minus_one_2_zero(test_label)\n",
        "\n",
        "  train_label = np.array(train_label).flatten()\n",
        "  val_label = np.array(val_label).flatten()\n",
        "  test_label = np.array(test_label).flatten()\n",
        "\n",
        "  train_index = np.array(train_index).flatten()\n",
        "  val_index = np.array(val_index).flatten()\n",
        "  test_index = np.array(test_index).flatten()\n",
        "\n",
        "  labels_set.append((train_label,val_label,test_label))\n",
        "  indexes_set.append((train_index,val_index,test_index))\n",
        "\n",
        "  train_index_all = train_chosen_idx\n",
        "  test_index_all = test_idx\n",
        "\n",
        "  subj = 0\n",
        "  return data, subj, indexes_set, labels_set, train_index_all, test_index_all, convert_minus_one_2_zero(labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxgb3ofX_QQq",
        "colab_type": "text"
      },
      "source": [
        "### Train CP Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdWoOubt_Nkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/ahwillia/tensortools\n",
        "\n",
        "import tensortools as tt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "def train_U_CP(data_name, R):\n",
        "  if(data_name == 'BP'):\n",
        "    x = scipy.io.loadmat('BP.mat')\n",
        "    X_normalize = x['X_normalize']\n",
        "\n",
        "  elif(data_name == 'HIV'):\n",
        "    x = scipy.io.loadmat('HIV.mat') \n",
        "    X_normalize = x['X'] \n",
        "\n",
        "  elif(data_name == 'PPMI'):\n",
        "    x = scipy.io.loadmat('PPMI.mat') \n",
        "    X_normalize = x['X'] \n",
        "\n",
        "  N = X_normalize.size\n",
        "  X_1 = X_normalize[0][0]\n",
        "  M,F,V = X_1.shape\n",
        "  # print('loading data:',data_name,'of size:',N,V,M,F)\n",
        "  data = np.zeros([M,F,V,N])\n",
        "  for i in range(N):\n",
        "    X_i = X_normalize[i][0]\n",
        "    for j in range(V):\n",
        "      X_ij = X_i[:,:,j]\n",
        "      data[:,:,j,i] = X_ij\n",
        "\n",
        "  data_select = data\n",
        "  F = tt.cp_als(data_select, rank=R, verbose=False)\n",
        "\n",
        "  U = F.factors[0]\n",
        "  \n",
        "\n",
        "  return U"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qyFkRdMRsSE",
        "colab_type": "text"
      },
      "source": [
        "### Train HOSVD Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5o8y7a4SKAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorly\n",
        "import tensorly as tl\n",
        "import time\n",
        "from  scipy import *\n",
        "\n",
        "\n",
        "def train_U(data_name, R):\n",
        "  if(data_name == 'BP'):\n",
        "    x = scipy.io.loadmat('BP.mat')\n",
        "    X_normalize = x['X_normalize']\n",
        "\n",
        "  elif(data_name == 'HIV'):\n",
        "    x = scipy.io.loadmat('HIV.mat') \n",
        "    X_normalize = x['X'] \n",
        "\n",
        "  elif(data_name == 'PPMI'):\n",
        "    x = scipy.io.loadmat('PPMI.mat') \n",
        "    X_normalize = x['X'] \n",
        "\n",
        "  N = X_normalize.size\n",
        "  X_1 = X_normalize[0][0]\n",
        "  M,F,V = X_1.shape\n",
        "  # print('loading data:',data_name,'of size:',N,V,M,F)\n",
        "  data = np.zeros([N,M,F,V])\n",
        "  for i in range(N):\n",
        "    X_i = X_normalize[i][0]\n",
        "    for j in range(V):\n",
        "      X_ij = X_i[:,:,j]\n",
        "      data[i,:,:,j] = X_ij\n",
        "\n",
        "  data_select = data\n",
        "  X1 = tl.unfold(data_select, mode=1)\n",
        "  B = np.matmul(X1,X1.transpose())\n",
        "  U, S, V = np.linalg.svd(B, full_matrices=True)\n",
        "  S = np.sqrt(S)\n",
        "  sum_all_S = np.sum(S)\n",
        "  len_S = len(S)\n",
        "  sum_part_S = 0\n",
        "  for i in range(len_S):\n",
        "    sum_part_S = sum_part_S + S[i]\n",
        "    if(sum_part_S>sum_all_S*R):\n",
        "        break\n",
        "\n",
        "  U = U[:,0:i]\n",
        "  return U\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGeFUngIaHb2",
        "colab_type": "text"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBMDiRvgAwHq",
        "colab_type": "code",
        "outputId": "7b46584c-db54-4805-f293-ee9058968af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print (\"Local time :\", localtime)\n",
        "print('##$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$###############$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$*****************')\n",
        "\n",
        "method = 'gcn'\n",
        "view_com = 'max'\n",
        "n_views = 2\n",
        "data_name = 'HIV'\n",
        "\n",
        "if(data_name == 'HIV'):\n",
        "  n_views = 2\n",
        "  num_sample = 70\n",
        "elif(data_name == 'BP'):\n",
        "  n_views = 2\n",
        "  num_sample = 97\n",
        "\n",
        "index_set_all = np.array(range(num_sample))\n",
        "R = 0.98\n",
        "data_type = str\n",
        "i_fold = 1\n",
        "n_epoch = 30\n",
        "batch_size = 1\n",
        "accuracy_all = 0\n",
        "accuracy_record = []\n",
        "param_record = {}\n",
        "iter = 0\n",
        "kfold = 10\n",
        "\n",
        "accuracy_total_sum = 0\n",
        "auc_total_sum = 0\n",
        "sensitivity_total_sum = 0\n",
        "specificity_total_sum = 0\n",
        "\n",
        "accuracy_total_record = []\n",
        "auc_total_record = []\n",
        "sensitivity_total_record = []\n",
        "specificity_total_record = []\n",
        "\n",
        "node_features = train_U_CP(data_name, 50)\n",
        "\n",
        "\n",
        "\n",
        "for train_fold_chosen in range(kfold):\n",
        "  print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "  print('start training the model of fold:',train_fold_chosen)\n",
        "  print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "  data, subj, indexes_set, labels_set, train_index_all, test_index_all, label_all = load_data_my_new(data_name, train_fold_chosen)\n",
        "  iter = 0\n",
        "  indexes = indexes_set[0]\n",
        "  labels = labels_set[0]\n",
        "  accuracy_record = []\n",
        "  param_record = {}\n",
        "  label_all = label_all[:,0]\n",
        "  mode = 'validate'\n",
        "\n",
        "\n",
        "  for m in range(80,100,20):\n",
        "    for k in range(8,10,2):\n",
        "      for batch_size in range(2,4,2):\n",
        "        theta = k\n",
        "        accuracy, auc, sensitivity, specificity = train(method, view_com, n_views, k, theta, m, n_epoch, batch_size, indexes, labels, node_features, subj, data, data_type, train_fold_chosen, mode, label_all)\n",
        "        print('validate_accuracy = ',accuracy)\n",
        "        accuracy_record.append(accuracy)\n",
        "        param_set = [k, m, n_epoch, batch_size, theta]\n",
        "        param_record[iter] = param_set\n",
        "        print('****************************************')\n",
        "\n",
        "        print('The input parameter is: ','k = ',k,'m = ',m,'n_epoch = ',n_epoch, 'batch_size= ',batch_size, 'theta_size = ',theta)\n",
        "        iter += 1\n",
        "\n",
        "\n",
        "  max_accuracy = max(accuracy_record)\n",
        "  max_index = np.argmax(np.array(accuracy_record))\n",
        "  params = param_record[max_index]\n",
        "  k = params[0]\n",
        "  m = params[1]\n",
        "  n_epoch = params[2]\n",
        "  batch_size = params[3]\n",
        "  theta = params[4]\n",
        "  print('max_accuracy is: ', max_accuracy, 'The input parameter is: ','k = ',k,'m = ',m,'n_epoch = ',n_epoch, 'batch_size= ',batch_size, 'theta_size = ',theta)\n",
        "\n",
        "\n",
        "# Retrain the model using all index\n",
        "  print('start retraining the model of fold:',train_fold_chosen)\n",
        "  print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "  test_index = test_index_all[train_fold_chosen]\n",
        "  test_label = label_all[test_index]\n",
        "  # test_label = test_label[:,0]\n",
        "  print(test_label.shape)\n",
        "  val_index, val_label = test_index, test_label\n",
        "  train_index = train_index_all\n",
        "  train_label = label_all[train_index]\n",
        "  # train_label = train_label[:,0]\n",
        "  print(train_label.shape)\n",
        "\n",
        "  mode = 'test'\n",
        "\n",
        "  labels_set = [train_label, val_label, test_label]\n",
        "  indexes_set = [train_index, val_index, test_index]\n",
        "  accuracy, auc, sensitivity, specificity  = train(method, view_com, n_views, k, theta, m, n_epoch, batch_size, indexes_set, labels_set, node_features, subj, data, data_type, i_fold, mode, label_all)\n",
        "  print('fold: ',train_fold_chosen, 'ccuracy = ',accuracy, 'auc = ',auc, 'sensitivity = ', sensitivity, 'specificity = ',specificity)\n",
        "\n",
        "  accuracy_total_sum = accuracy_total_sum + accuracy\n",
        "  auc_total_sum = auc_total_sum + auc\n",
        "  sensitivity_total_sum = sensitivity_total_sum + sensitivity\n",
        "  specificity_total_sum = specificity_total_sum + specificity\n",
        "\n",
        "\n",
        "  accuracy_total_record.append(accuracy)\n",
        "  auc_total_record.append(auc)\n",
        "  sensitivity_total_record.append(sensitivity)\n",
        "  specificity_total_record.append(specificity)\n",
        "  print('****************************************')\n",
        "\n",
        "print('**********************')\n",
        "average_accuracy = accuracy_total_sum/(train_fold_chosen+1)\n",
        "average_auc = auc_total_sum/(train_fold_chosen+1)\n",
        "average_sensitivity = sensitivity_total_sum/(train_fold_chosen+1)\n",
        "average_specificity = specificity_total_sum/(train_fold_chosen+1)\n",
        "/\n",
        "std_value = np.std(accuracy_total_record, ddof = 1)\n",
        "print('accuracy_all = ', accuracy_total_record)\n",
        "print('auc_all = ', auc_total_record)\n",
        "print('sensitivity_all = ', sensitivity_total_record)\n",
        "print('specificity_all = ', specificity_total_record)\n",
        "\n",
        "print('view_com = ', view_com, 'test_data', data_name, 'average_accuracy = ',average_accuracy, 'average_auc = ',average_auc, 'average_sensitivity = ',average_sensitivity, 'average_specificity= ', average_specificity, 'std_value = ', std_value)\n",
        "print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local time : Sun Jun 14 22:55:49 2020\n",
            "##$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$###############$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$*****************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 0\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.3846153846153846\n",
            "validate_accuracy =  0.3846153846153846\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.3846153846153846 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 0\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.8571428571428572\n",
            "[0. 0. 1. 0. 1. 0. 0.]\n",
            "fold:  0 ccuracy =  0.8571428571428572 auc =  83.33333333333333 sensitivity =  0.6666666666666666 specificity =  1.0\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 1\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.3076923076923077\n",
            "validate_accuracy =  0.3076923076923077\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.3076923076923077 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 1\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.7142857142857143\n",
            "[0. 0. 0. 1. 0. 0. 1.]\n",
            "fold:  1 ccuracy =  0.7142857142857143 auc =  65.00000000000001 sensitivity =  0.5 specificity =  0.8\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 2\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.15384615384615385\n",
            "validate_accuracy =  0.15384615384615385\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.15384615384615385 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 2\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.8571428571428572\n",
            "[0. 0. 1. 1. 0. 0. 1.]\n",
            "fold:  2 ccuracy =  0.8571428571428572 auc =  87.5 sensitivity =  0.75 specificity =  1.0\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 3\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.3846153846153846\n",
            "validate_accuracy =  0.3846153846153846\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.3846153846153846 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 3\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.7142857142857143\n",
            "[1. 1. 0. 0. 0. 1. 0.]\n",
            "fold:  3 ccuracy =  0.7142857142857143 auc =  70.83333333333334 sensitivity =  0.6666666666666666 specificity =  0.75\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 4\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.46153846153846156\n",
            "validate_accuracy =  0.46153846153846156\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.46153846153846156 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 4\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.8571428571428572\n",
            "[0. 1. 1. 1. 1. 1. 1.]\n",
            "fold:  4 ccuracy =  0.8571428571428572 auc =  75.0 sensitivity =  1.0 specificity =  0.5\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 5\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.5384615384615384\n",
            "validate_accuracy =  0.5384615384615384\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.5384615384615384 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 5\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.4285714285714286\n",
            "[1. 0. 1. 0. 0. 0. 1.]\n",
            "fold:  5 ccuracy =  0.4285714285714286 auc =  41.666666666666664 sensitivity =  0.3333333333333333 specificity =  0.5\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 6\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.6153846153846154\n",
            "validate_accuracy =  0.6153846153846154\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.6153846153846154 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 6\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.5714285714285714\n",
            "[0. 0. 0. 0. 0. 0. 0.]\n",
            "fold:  6 ccuracy =  0.5714285714285714 auc =  50.0 sensitivity =  0.0 specificity =  1.0\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 7\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.3846153846153846\n",
            "validate_accuracy =  0.3846153846153846\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.3846153846153846 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 7\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.4285714285714286\n",
            "[1. 1. 1. 1. 1. 1. 0.]\n",
            "fold:  7 ccuracy =  0.4285714285714286 auc =  60.0 sensitivity =  1.0 specificity =  0.2\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 8\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.23076923076923073\n",
            "validate_accuracy =  0.23076923076923073\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.23076923076923073 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 8\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.5714285714285714\n",
            "[0. 0. 0. 0. 0. 0. 0.]\n",
            "fold:  8 ccuracy =  0.5714285714285714 auc =  50.0 sensitivity =  0.0 specificity =  1.0\n",
            "****************************************\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "start training the model of fold: 9\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "loading data: HIV of size: 70 2 90 90\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-750\n",
            "accuracy =  0.5384615384615384\n",
            "validate_accuracy =  0.5384615384615384\n",
            "****************************************\n",
            "The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "max_accuracy is:  0.5384615384615384 The input parameter is:  k =  8 m =  80 n_epoch =  30 batch_size=  2 theta_size =  8\n",
            "start retraining the model of fold: 9\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "(7,)\n",
            "(63,)\n",
            "Set parameters...\n",
            "Get feed indexes and labels...\n",
            "****** start_testing *******\n",
            "80 [80]\n",
            "INFO:tensorflow:Restoring parameters from models/checkpoints/HIV/mvgcn/model-945\n",
            "accuracy =  0.7142857142857143\n",
            "[0. 1. 0. 0. 0. 1. 0.]\n",
            "fold:  9 ccuracy =  0.7142857142857143 auc =  75.0 sensitivity =  0.5 specificity =  1.0\n",
            "****************************************\n",
            "**********************\n",
            "accuracy_all =  [0.8571428571428572, 0.7142857142857143, 0.8571428571428572, 0.7142857142857143, 0.8571428571428572, 0.4285714285714286, 0.5714285714285714, 0.4285714285714286, 0.5714285714285714, 0.7142857142857143]\n",
            "auc_all =  [83.33333333333333, 65.00000000000001, 87.5, 70.83333333333334, 75.0, 41.666666666666664, 50.0, 60.0, 50.0, 75.0]\n",
            "sensitivity_all =  [0.6666666666666666, 0.5, 0.75, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.5]\n",
            "specificity_all =  [1.0, 0.8, 1.0, 0.75, 0.5, 0.5, 1.0, 0.2, 1.0, 1.0]\n",
            "view_com =  max test_data HIV average_accuracy =  0.6714285714285715 average_auc =  65.83333333333334 average_sensitivity =  0.5416666666666666 average_specificity=  0.775 std_value =  0.1656431155326294\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}